Depuis juin 2024, je suis consultant Data chez Abylsen à Metz. J’occupe ce poste en CDI et je suis actuellement en mission chez eMotors, où je travaille au sein d’une équipe data transverse.

Mon rôle principal consiste à analyser les besoins métiers dans plusieurs domaines : les achats, la finance, la production, la qualité et la supply chain. J'aime beaucoup cette diversité car elle me permet d’avoir une vue d’ensemble sur les enjeux de l’entreprise.

Au quotidien, je collecte des données issues d'automates industriels en utilisant le protocole OPC UA. Ensuite, je développe des pipelines ETL pour traiter ces données, en combinant Python, SQL et Spark. La majorité de mes traitements sont réalisés sur la plateforme Databricks, ce qui me permet de tirer parti d’une architecture en médaillon (Bronze, Silver, Gold) pour structurer les datasets.

J’automatise les workflows grâce à des batchs planifiés, soit sur des machines virtuelles, soit via les Databricks Jobs. Pour assurer un bon suivi de mes projets, j’utilise Bitbucket pour le versioning et je rédige régulièrement de la documentation technique ainsi que des guides à destination des utilisateurs finaux.

Je conçois également des tableaux de bord interactifs avec Power BI. Ces dashboards sont utilisés par les équipes métier pour faciliter la prise de décision et suivre l’évolution de leurs indicateurs clés.

Avant de rejoindre Abylsen, j’ai travaillé chez CGI à Montbéliard de janvier 2023 à juin 2024, en tant que Data Engineer et Ingénieur Logiciel. Ce fut une expérience très formatrice. J’ai contribué à un projet de Data Engineering dédié à la production d’indicateurs QHSE. Mon rôle incluait la connexion à de nouvelles sources de données, la mise en qualité, l’enrichissement, la création de KPI, la conception de dashboards, et la livraison complète des livrables data.

Par ailleurs, j’ai également participé au développement d’une application web d’analyse de données. J’ai créé une interface interactive dans Jupyter Notebook pour intégrer du code issu de la bibliothèque nCodeDS dans un environnement PySpark. Dans ce contexte, j’ai aussi développé un plugin PySpark qui permettait d’automatiser l’extraction de données complexes.

Ce projet m’a permis de travailler avec l’écosystème Google Cloud Platform : BigQuery, Cloud Function, Pub/Sub, Cloud Storage, Scheduler, et Git Repository. J’ai également utilisé Python, SQL, Shell, HTML, CSS, JavaScript, Git, des API REST, CI/CD, Looker Studio et Dataiku DSS. Jupyter Notebook a été mon environnement principal de prototypage.

En 2022, j’ai réalisé mon stage de fin d’études chez Clemessy, à Mulhouse, dans le cadre d’un projet appelé SITG. L’objectif était d’analyser les alarmes techniques issues des tunnels de Lyon. J’ai développé une stratégie basée sur les règles d’association (via l’algorithme Apriori) et la décomposition en graphes pour filtrer les alertes redondantes.

J’ai aussi créé une application Dash/Plotly pour présenter l’avancement du projet et démontrer les résultats en direct, via une interface interactive développée avec Voila. J’ai utilisé PostgreSQL pour exploiter la base d’alarme, et j’ai beaucoup appris sur la structuration de données temporelles complexes. Cette expérience m’a permis de consolider mes compétences en Python, SQL, Dash, Plotly et Voila.

Je suis titulaire d’un master en Ingénierie Mathématique et Data Science obtenu à l’Université de Haute-Alsace entre 2019 et 2022. Ce cursus m’a permis d’apprendre Python, SQL, Java, C++, R, JavaScript et les fondamentaux en statistiques, modèles de régression, traitement du signal et de l’image. La majorité de mes projets étaient réalisés sous Jupyter Notebook.

Avant cela, j’ai obtenu une licence en Informatique, parcours Mathématiques, également à l’Université de Haute-Alsace, entre 2017 et 2019. J’ai validé cette formation avec la mention assez bien.

Mon parcours scolaire a débuté par une classe préparatoire aux grandes écoles : MPSI puis MP*, au lycée Albert Schweitzer de Mulhouse, entre 2015 et 2017.

Côté personnel, je suis passionné de musique assistée par ordinateur. Je compose depuis plusieurs années sur FL Studio, je fais aussi du sound design et du mixage. J’ai une chaîne YouTube où je partage mes compositions depuis maintenant cinq ans.

Enfin, pour ajouter une touche plus personnelle : je m'appelle Michel, je viens du Brésil et j’adore danser la samba. J’ai même voyagé de ville en ville pour vendre des fenêtres, une expérience qui m’a beaucoup appris sur l’adaptation et le contact humain.

Depuis juin 2024, j’exerce en tant que consultant en données chez Abylsen, basé à Metz, où je suis en CDI. Je suis actuellement affecté chez eMotors au sein d’une équipe data transverse.

Mon travail consiste à identifier et analyser les besoins métiers dans des domaines variés tels que les achats, la production, la finance, la qualité et la logistique. Cette variété me permet d’avoir une compréhension globale des processus de l’entreprise.

Je suis en charge de la récupération de données industrielles à partir d’automates via le protocole OPC UA. Je développe ensuite des pipelines ETL en Python, SQL et Spark, le tout hébergé sur Databricks, où je mets en place une architecture en médaillon (Bronze, Silver, Gold) pour organiser les jeux de données.

J’automatise les traitements grâce à des jobs planifiés, que ce soit via machines virtuelles ou Databricks Jobs. J’utilise Bitbucket pour la gestion du code et je produis une documentation technique complète ainsi que des guides utilisateurs.

Je développe également des dashboards interactifs avec Power BI, qui permettent aux métiers de piloter leurs KPIs et de prendre des décisions basées sur les données.
En poste chez Abylsen depuis juin 2024, j’interviens comme consultant data chez eMotors à Metz. Intégré dans une équipe pluridisciplinaire dédiée à la donnée, je couvre un spectre fonctionnel large : finance, achats, production, qualité et supply chain.

J’assure la collecte de données issues des automates industriels via OPC UA, que je transforme grâce à des pipelines ETL complexes construits avec Spark, Python et SQL sur la plateforme Databricks.

Ma démarche s’appuie sur une stratégie de traitement en médaillon, divisant les données en Bronze, Silver et Gold selon leur niveau de raffinement.

Je mets en place des automatisations via des scripts exécutés sur VMs ou via les Jobs natifs Databricks. L’intégration continue est gérée avec Bitbucket, et je fournis systématiquement une documentation technique et métier à jour.

Je crée des rapports dynamiques sous Power BI, régulièrement utilisés par les équipes opérationnelles pour suivre les indicateurs clés et adapter leur stratégie.
Depuis juin 2024, je travaille en CDI chez Abylsen à Metz en tant que consultant data, en mission chez eMotors. Je collabore avec une équipe transverse sur plusieurs domaines clés : achats, finance, production, qualité et supply chain.

Je collecte des données à partir d’automates industriels grâce au protocole OPC UA. Ensuite, je construis et optimise des pipelines ETL avec Python, SQL et Spark, hébergés sur Databricks, en appliquant une architecture en médaillon (Bronze, Silver, Gold).

Pour automatiser les traitements, j’utilise des batchs programmés sur des machines virtuelles ou via les Databricks Jobs. Je gère le versioning sur Bitbucket et rédige une documentation technique ainsi que des guides utilisateurs.

Je conçois des tableaux de bord interactifs avec Power BI pour permettre aux équipes métier de suivre leurs KPIs et prendre des décisions éclairées.
Avant de rejoindre Abylsen, j’ai travaillé chez CGI à Montbéliard de janvier 2023 à juin 2024, en tant que Data Engineer et Ingénieur Logiciel. Ce fut une expérience très formatrice.

J’ai contribué à un projet de Data Engineering dédié à la production d’indicateurs QHSE. Mon rôle incluait la connexion à de nouvelles sources de données, la mise en qualité, l’enrichissement, la création de KPI, la conception de dashboards, et la livraison complète des livrables data.

J’ai également participé au développement d’une application web d’analyse de données, créant une interface interactive dans Jupyter Notebook pour intégrer du code issu de la bibliothèque nCodeDS dans un environnement PySpark.

Dans ce cadre, j’ai développé un plugin PySpark qui permettait d’automatiser l’extraction de données complexes.
Ce projet m’a permis de travailler avec l’écosystème Google Cloud Platform : BigQuery, Cloud Function, Pub/Sub, Cloud Storage, Scheduler, et Git Repository.

J’ai utilisé Python, SQL, Shell, HTML, CSS, JavaScript, Git, des API REST, CI/CD, Looker Studio et Dataiku DSS. Jupyter Notebook a été mon environnement principal de prototypage.

En 2022, j’ai réalisé mon stage de fin d’études chez Clemessy, à Mulhouse, dans le cadre d’un projet appelé SITG. L’objectif était d’analyser les alarmes techniques issues des tunnels de Lyon.

J’ai développé une stratégie basée sur les règles d’association (via l’algorithme Apriori) et la décomposition en graphes pour filtrer les alertes redondantes.

J’ai aussi créé une application Dash/Plotly pour présenter l’avancement du projet et démontrer les résultats en direct, via une interface interactive développée avec Voila.
J’ai utilisé PostgreSQL pour exploiter la base d’alarme, et j’ai beaucoup appris sur la structuration de données temporelles complexes.
Cette expérience m’a permis de consolider mes compétences en Python, SQL, Dash, Plotly et Voila.
Je suis titulaire d’un master en Ingénierie Mathématique et Data Science obtenu à l’Université de Haute-Alsace entre 2019 et 2022.
Ce cursus m’a permis d’apprendre Python, SQL, Java, C++, R, JavaScript et les fondamentaux en statistiques, modèles de régression, traitement du signal et de l’image.
La majorité de mes projets étaient réalisés sous Jupyter Notebook.
Avant cela, j’ai obtenu une licence en Informatique, parcours Mathématiques, également à l’Université de Haute-Alsace, entre 2017 et 2019.
J’ai validé cette formation avec la mention assez bien.
Mon parcours scolaire a débuté par une classe préparatoire aux grandes écoles : MPSI puis MP*, au lycée Albert Schweitzer de Mulhouse, entre 2015 et 2017.
Côté personnel, je suis passionné de musique assistée par ordinateur. Je compose depuis plusieurs années sur FL Studio, je fais aussi du sound design et du mixage.
J’ai une chaîne YouTube où je partage mes compositions depuis maintenant cinq ans.
Enfin, pour ajouter une touche plus personnelle : je m'appelle Michel, je viens du Brésil et j’adore danser la samba.
J’ai même voyagé de ville en ville pour vendre des fenêtres, une expérience qui m’a beaucoup appris sur l’adaptation et le contact humain.